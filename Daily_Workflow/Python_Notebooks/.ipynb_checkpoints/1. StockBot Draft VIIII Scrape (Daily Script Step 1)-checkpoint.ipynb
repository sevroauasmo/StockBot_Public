{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c5f6b1-5874-4791-b051-b076fb529541",
   "metadata": {},
   "source": [
    "## Step 1: Get Index, Scrape Data Since Last Time We Scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4ca9da-c925-4854-87f6-ee34a0c71d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import edgar\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e836a12-fbaa-41ab-bda2-6ce6c10b0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9f761b-4c59-4053-ac34-659f8625790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ua = [First Name, Last Name, email]\n",
    "\n",
    "edgar.download_index('/Users/rileybitterli/Desktop/StockBot_Data/StockBot_Indices', 2023, ua, skip_all_present_except_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5659a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year = 2024\n",
      "quarter = QTR3\n"
     ]
    }
   ],
   "source": [
    "#Get Current Year and Quarter for daily scraping purposes\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_year_and_quarter():\n",
    "    current_date = datetime.now()\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "\n",
    "    if 1 <= month <= 3:\n",
    "        quarter = \"QTR1\"\n",
    "    elif 4 <= month <= 6:\n",
    "        quarter = \"QTR2\"\n",
    "    elif 7 <= month <= 9:\n",
    "        quarter = \"QTR3\"\n",
    "    else:\n",
    "        quarter = \"QTR4\"\n",
    "\n",
    "    return year, quarter\n",
    "\n",
    "year, quarter = get_current_year_and_quarter()\n",
    "print(f\"year = {year}\")\n",
    "print(f\"quarter = {quarter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22805e5-7d01-4903-af5c-a7b706b535fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rileybitterli/Desktop/StockBot_Data/StockBot_Indices/2024-QTR3.tsv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_path = f'/Users/rileybitterli/Desktop/StockBot_Data/StockBot_Indices/{year}-{quarter}.tsv'\n",
    "# Define a regex pattern to match the year and the number after QTR\n",
    "match = re.search(r'(\\d{4})-QTR(\\d)', file_path)\n",
    "\n",
    "if match:\n",
    "    filing_year = int(match.group(1))\n",
    "    filing_qtr = 'Q' + match.group(2)  # prepend 'Q' to the quarter number\n",
    "else:\n",
    "    print(\"Pattern not found in the provided string\")\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2224e0c-cb4b-4aac-80f6-abaf21c10c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = pd.read_csv(file_path, delimiter= '|', names = ('CIK', 'Name', 'Form_Type', 'Date', 'URL_TXT', 'URL_HTML'))\n",
    "\n",
    "index_df = index_df[index_df['Form_Type']=='4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cebcdb4-7d45-4d37-9eea-c0fe9363b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df['FileURL'] = 'https://sec.gov/Archives/' + index_df['URL_HTML']\n",
    "index_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "456b1a57-7eaa-47e8-9d52-abb2b0452c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_date(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            last_date = file.readline().strip()\n",
    "            return last_date\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def write_last_date(file_path, date):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f94df7dd-bfc4-4761-aa87-aa6a501cbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp file that's overwritten each day the script runs\n",
    "date_file_path = '/Users/rileybitterli/Documents/GitHub/SB_VIII_Streamlined/Daily_Workflow/Temp_Files/last_run_date.txt'\n",
    "\n",
    "# read the last stored date\n",
    "last_stored_date = read_last_date(date_file_path)\n",
    "\n",
    "#c onvert last_stored_date to datetime\n",
    "last_stored_date = pd.to_datetime(last_stored_date)\n",
    "\n",
    "# ensure the 'Date' column is datetime\n",
    "index_df['Date'] = pd.to_datetime(index_df['Date'])\n",
    "\n",
    "# filter the df\n",
    "since_last_run = index_df[index_df['Date'] >= last_stored_date]\n",
    "\n",
    "# Now, filtered_df contains only the rows with Date >= last_stored_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4adcf7-0d8c-49ff-acbe-73e8f47d6a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-07-29 00:00:00')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_stored_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "514d17fa-0af2-451b-ac45-ee1a8a1ebdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we've got a list of the urls since last time we scraped, as well as rewritten the new date to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05095d8-0ffc-4b85-bc37-5e044a5259b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import concurrent.futures\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup \n",
    "from random import uniform\n",
    "from time import sleep\n",
    "import lxml\n",
    "from bs4.formatter import HTMLFormatter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d762aeb9-a55f-49a1-ae88-72dd8c4902f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to send you emails- when the job completes, errors, etc.\n",
    "def send_email(subject, content):\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(content)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = 'halpitsstockbot@gmail.com'\n",
    "    msg['To'] = 'riley.bitterli@gmail.com'\n",
    "\n",
    "    # Establish a connection to Gmail\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    #server.login([insert email address], [insert password])\n",
    "    server.send_message(msg)\n",
    "    server.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "257d6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_path = ChromeDriverManager().install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db0e7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles weird edge case of chromedriver\n",
    "if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "    chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f08310fc-b51c-474f-a91c-9703c0e45a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(chrome_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18c21e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-background-timer-throttling\")\n",
    "chrome_options.add_argument(\"--disable-backgrounding-occluded-windows\")\n",
    "chrome_options.add_argument(\"--disable-renderer-backgrounding\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-gpu\") \n",
    "chrome_options.add_argument(\"--enable-accelerated-2d-canvas\")\n",
    "chrome_options.add_argument(\"--ignore-gpu-blacklist\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # This can help performance but reduces security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86832e29",
   "metadata": {},
   "source": [
    "## Step 2: Cut Insider Trading Links into Chunks for MultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a45874b-e9b3-4926-bb1b-2f62ace59c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DataFrame columns into a list of tuples\n",
    "url_date_pairs = list(zip(since_last_run['FileURL'], since_last_run['Date']))\n",
    "\n",
    "# define  chunk size\n",
    "chunk_size = 100\n",
    "\n",
    "# create chunks\n",
    "chunks = list(enumerate([url_date_pairs[i:i + chunk_size] for i in range(0, len(url_date_pairs), chunk_size)]))\n",
    "\n",
    "# now, `chunks` is a list of indexed tuples, where each tuple contains an index and a list of up to 100 URL-Date pairings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3875e66c-7b80-4010-965b-7c79a2aee508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2da0c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pull_string = last_stored_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79f25725",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now()\n",
    "\n",
    "date_of_scrape = current_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e88ee742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-08'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7322366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: /Users/rileybitterli/Documents/GitHub/SB_VIII_Streamlined/Daily_Workflow/Daily_Scrape_Pickles/2024-07-29-->2024-08-08/\n"
     ]
    }
   ],
   "source": [
    "folder_path = f\"/Users/rileybitterli/Documents/GitHub/SB_VIII_Streamlined/Daily_Workflow/Daily_Scrape_Pickles/{last_pull_string}-->{date_of_scrape}/\"\n",
    "\n",
    "# check if the folder exists, and create it if it doesn't\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder created: {folder_path}\")\n",
    "else:\n",
    "    print(f\"Folder already exists: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30343d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I slept for: 0.69 seconds, and my index is: 1 url counter is: 0 batch number 0 chunk_number: 2\n",
      "I slept for: 0.82 seconds, and my index is: 1 url counter is: 0 batch number 1 chunk_number: 1\n",
      "I slept for: 0.57 seconds, and my index is: 1 url counter is: 0 batch number 2 chunk_number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## This the biggest lift- we're multiprocessing the links to speed up pull time, pulling data we need, and saving it\n",
    "\n",
    "import threading\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "\n",
    "# Assuming 's', 'send_email', 'filing_year', 'filing_qtr', and 'chunks' are defined elsewhere\n",
    "\n",
    "# For thread safety\n",
    "batch_number_counter = [0]  # Using a list so it's mutable\n",
    "batch_number_lock = threading.Lock()\n",
    "\n",
    "def scrape_chunk(chunk_number, chunk):\n",
    "    global batch_number_counter\n",
    "    necessary_dfs = []  # Corrected list name for consistency\n",
    "    # Setup a new webdriver instance\n",
    "   # driver = webdriver.Chrome(service=s, options = chrome_options)\n",
    "    driver = webdriver.Chrome(service = s, options = chrome_options)\n",
    "    driver.delete_all_cookies()\n",
    "    index = 0\n",
    "    url_counter = 0\n",
    "    with batch_number_lock:\n",
    "        batch_number = batch_number_counter[0]\n",
    "        batch_number_counter[0] += 1\n",
    "\n",
    "    for url, date in chunk:\n",
    "        index += 1\n",
    "        try: \n",
    "            sec = uniform(.4,1)\n",
    "            print(\"I slept for:\", round(sec, 2), \"seconds, and my index is:\", index, \"url counter is:\", url_counter, 'batch number', batch_number, \"chunk_number:\", chunk_number)\n",
    "            driver.get(url)\n",
    "            sleep(sec)\n",
    "            web_elements = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            links = [we.get_attribute(\"href\") for we in web_elements if we.get_attribute(\"href\")]\n",
    "            html_links = [link for link in links if 'xml' in link]\n",
    "    \n",
    "            if html_links:\n",
    "                driver.get(html_links[0])\n",
    "                full_page = driver.page_source.replace('%', '')\n",
    "                dfs = pd.read_html(full_page)\n",
    "\n",
    "        \n",
    "                for df in dfs:\n",
    "                   if isinstance(df, pd.DataFrame):\n",
    "                    try:\n",
    "                        # Ensure we're working with a string for the regex search\n",
    "                        first_item = str(df.iloc[0, 1]) if not df.empty and df.columns.size > 1 else \"\"\n",
    "                        bracket_match = re.search(r'\\[(.*?)\\]', first_item)\n",
    "                        if bracket_match:\n",
    "                            ticker = bracket_match.group(1)  # Extract the value between the brackets\n",
    "                            name = df.iloc[1, 1]\n",
    "                    except Exception as e:\n",
    "                        # If any error occurs, ignore and move on\n",
    "                        pass \n",
    "\n",
    "                    if isinstance(df.columns, pd.MultiIndex):\n",
    "                        for col in df.columns.levels[0]:\n",
    "                            if col.startswith('Table I - Non-Derivative Securities Acquired, Disposed of, or Beneficially Owned'):\n",
    "                                df.columns = df.columns.droplevel(0).droplevel(0)\n",
    "                                df.rename(columns={\n",
    "                                        '1. Title of Security (Instr. 3)': 'security_type',\n",
    "                                        '2. Transaction Date  (Month/Day/Year)': 'date',\n",
    "                                        '2A. Deemed Execution Date, if any  (Month/Day/Year)': 'execution_date',\n",
    "                                        'Code': 'code',\n",
    "                                        'V': 'v',\n",
    "                                        '(A) or (D)': 'a_or_d',\n",
    "                                        'Amount': 'amount',\n",
    "                                        '5. Amount of Securities Beneficially Owned Following Reported Transaction(s) (Instr. 3 and 4)': 'total_owned_after_trans',\n",
    "                                        '6. Ownership Form: Direct (D) or Indirect (I) (Instr. 4)': 'direct_or_indirect'\n",
    "                                    }, inplace=True)\n",
    "\n",
    "                                df['URL'] = url\n",
    "                                df['Filing Date'] = date\n",
    "                                df['Ticker'] = ticker\n",
    "                                df['Name'] = name\n",
    "\n",
    "                                if 'code' in df.columns and (df['code'].eq('P').any() or df['code'].eq('S').any()):\n",
    "                                    necessary_dfs.append(df)\n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "                url_counter += 1\n",
    "                if url_counter == 1000:\n",
    "                     with open(f\"{folder_path}batch{batch_number}.pkl\", \"wb\") as file:\n",
    "                        pickle.dump(necessary_dfs, file)\n",
    "                        necessary_dfs = []\n",
    "                        url_counter = 0\n",
    "                        \n",
    "                        \n",
    "        except IndexError as ie:\n",
    "            print(f\"Skipped URL due to error at index:{index}. Error: {str(ie)}\")\n",
    "            continue\n",
    "                        \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Document is empty\" in str(e):\n",
    "                print(f\"Skipped URL due to error at index:{index}. Error: {str(e)}\")\n",
    "                continue\n",
    "            else:\n",
    "                send_email('Error in Jupyter Notebook', f\"Broke at index:{index} An error occurred: {str(e)}\")\n",
    "                raise e\n",
    "\n",
    "    if necessary_dfs:\n",
    "        with open(f\"{folder_path}batch{batch_number}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(necessary_dfs, file)\n",
    "            necessary_dfs = []\n",
    "            url_counter = 0\n",
    "    print('all done')\n",
    "    driver.close() \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(scrape_chunk, chunk[0], chunk[1]) for chunk in chunks]\n",
    "        concurrent.futures.wait(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9571804-243d-453b-b02a-6339539b39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the scraping is done, re-set the most recent date to be the day of the scraping (today)\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "write_last_date(date_file_path, current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e907dc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rileybitterli/Documents/GitHub/SB_VIII_Streamlined/Daily_Workflow/Daily_Scrape_Pickles/2024-06-25-->2024-06-25/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f764fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp file to reference in next step\n",
    "file_path = \"/Users/rileybitterli/Documents/GitHub/SB_VIII_Streamlined/Daily_Workflow/Temp_Files/step_1_to_2.txt\"\n",
    "\n",
    "\n",
    "# create or overwrite the file and write the contents of the string variable to it\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8557452",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email('completed', f'step 1 of daily for{current_date}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
